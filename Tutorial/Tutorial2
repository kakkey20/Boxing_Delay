1. ライブラリのインポート:
gymnasium（gymとしてインポート）: RLアルゴリズムのための環境ライブラリ。
stable_baselines3: RLアルゴリズムの実装を含むライブラリ。
os, numpyなど: 汎用的な機能を提供するライブラリ。

2. モデルのトレーニングと保存:
Proximal Policy Optimization（PPO）とAdvantage Actor-Critic（A2C）アルゴリズムを使用して、Pendulum環境でモデルをトレーニング。
トレーニングされたモデルをディスクに保存し、後でロードして使用。

3. カスタム環境ラッパーの作成:
様々なカスタムラッパーを定義して環境を拡張。これにはCustomWrapper, TimeLimitWrapper, NormalizeActionWrapperが含まれる。
これらのラッパーは、環境のリセット、ステップ関数をカスタマイズし、追加機能（例：アクションの正規化、エピソードの時間制限）を提供。

4. 環境の操作と観察:
Pendulum-v1とLunarLander-v2環境でランダムアクションを実行し、環境の動作を観察。
TimeFeatureWrapperを使用して、エピソードの残り時間を観測に追加。

5. 環境のモニタリングと正規化:
MonitorラッパーとDummyVecEnvを使用して環境の監視とベクトル化。
VecNormalizeを使用して観測と報酬を正規化。

6. モデルのさらなるトレーニング:
異なるラッパーを適用した環境で、A2Cアルゴリズムを使用してモデルをトレーニング。

7. モデルの保存ファイルの探索:
保存されたモデルファイルを解凍し、その内容を表示。
