1. モデルの評価とチューニング:
Pendulum-v1環境でSoft Actor-Critic (SAC) アルゴリズムを使用して2つのモデル（デフォルトとチューニングされたモデル）をトレーニング。
両モデルを100エピソードで評価し、平均報酬と標準偏差を出力。

2. カスタムコールバックの定義と使用:
BaseCallbackを継承した複数のカスタムコールバック（CustomCallback, SimpleCallback, SaveOnBestTrainingRewardCallback, PlottingCallback, ProgressBarCallback）を定義。
これらのコールバックはトレーニング中の異なるイベント（例：ステップごとの進行、パフォーマンスに基づくモデルの保存、リアルタイムでのパフォーマンスのプロット）に対応している。

3. コールバックを使用したモデルのトレーニング:
ProgressBarCallbackを使用してTD3アルゴリズムのトレーニング進捗を表示。
CallbackListを使用して、複数のコールバック（進捗バーと自動保存）を組み合わせてPPOアルゴリズムのトレーニングを実行。

4. 評価コールバックの作成:
EvalCallbackクラスを定義し、定期的にエージェントを評価する仕組みを提供。
このクラスは、特定の頻度でエージェントを評価し、パフォーマンスに基づいて最良のモデルを保存する機能を持つ。

5. 環境の設定とモニタリング:
make_vec_envとMonitorラッパーを使用して、トレーニング用の環境を作成し、パフォーマンスデータを記録。
